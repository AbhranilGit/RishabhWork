{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Load service account credentials\n",
    "SERVICE_ACCOUNT_FILE = \"path/to/your-service-account.json\"\n",
    "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE, scopes=SCOPES\n",
    ")\n",
    "\n",
    "# Build the Search Console API client\n",
    "service = build(\"searchconsole\", \"v1\", credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating...\n",
      "\n",
      "Available sites:\n",
      "- sc-domain:scholistico.com\n",
      "\n",
      "Fetching all data for sc-domain:scholistico.com\n",
      "Date range: 2024-10-11 to 2025-01-09\n",
      "Dimensions: date, query, page, device, country\n",
      "Fetched 25000 rows (Total: 25000)\n",
      "Fetched 25000 rows (Total: 50000)\n",
      "Fetched 25000 rows (Total: 75000)\n",
      "Fetched 25000 rows (Total: 100000)\n",
      "Fetched 25000 rows (Total: 125000)\n",
      "Fetched 25000 rows (Total: 150000)\n",
      "Fetched 25000 rows (Total: 175000)\n",
      "Fetched 25000 rows (Total: 200000)\n",
      "Fetched 25000 rows (Total: 225000)\n",
      "Fetched 25000 rows (Total: 250000)\n",
      "Fetched 25000 rows (Total: 275000)\n",
      "Fetched 25000 rows (Total: 300000)\n",
      "Fetched 25000 rows (Total: 325000)\n",
      "Fetched 25000 rows (Total: 350000)\n",
      "Fetched 25000 rows (Total: 375000)\n",
      "Fetched 25000 rows (Total: 400000)\n",
      "Fetched 25000 rows (Total: 425000)\n",
      "Fetched 25000 rows (Total: 450000)\n",
      "Fetched 25000 rows (Total: 475000)\n",
      "Fetched 25000 rows (Total: 500000)\n",
      "Fetched 25000 rows (Total: 525000)\n",
      "Fetched 25000 rows (Total: 550000)\n",
      "Fetched 25000 rows (Total: 575000)\n",
      "Fetched 25000 rows (Total: 600000)\n",
      "Fetched 25000 rows (Total: 625000)\n",
      "Fetched 25000 rows (Total: 650000)\n",
      "Fetched 25000 rows (Total: 675000)\n",
      "Fetched 25000 rows (Total: 700000)\n",
      "Fetched 25000 rows (Total: 725000)\n",
      "Fetched 25000 rows (Total: 750000)\n",
      "Fetched 25000 rows (Total: 775000)\n",
      "Fetched 25000 rows (Total: 800000)\n",
      "Fetched 25000 rows (Total: 825000)\n",
      "Fetched 25000 rows (Total: 850000)\n",
      "Fetched 25000 rows (Total: 875000)\n",
      "Fetched 4135 rows (Total: 879135)\n",
      "\n",
      "Total rows fetched: 879135\n",
      "\n",
      "Processing data...\n",
      "\n",
      "Saving data to search_console_data_2024-10-11_to_2025-01-09.csv...\n",
      "\n",
      "Data Summary:\n",
      "Total rows: 879135\n",
      "\n",
      "Sample of the data:\n",
      "         date        query                         page   device country  \\\n",
      "0  2024-11-23  scholistico  https://de.scholistico.com/   MOBILE     deu   \n",
      "1  2024-11-27  scholistico  https://de.scholistico.com/   MOBILE     deu   \n",
      "2  2024-12-03  scholistico     https://scholistico.com/  DESKTOP     usa   \n",
      "3  2024-11-26  scholistico  https://de.scholistico.com/   MOBILE     deu   \n",
      "4  2024-11-22  scholistico  https://de.scholistico.com/   MOBILE     deu   \n",
      "\n",
      "   clicks  impressions       ctr  position  \n",
      "0      23           37  0.621622       1.0  \n",
      "1      19           23  0.826087       1.0  \n",
      "2      19           53  0.358491       1.0  \n",
      "3      18           27  0.666667       1.0  \n",
      "4      17           27  0.629630       1.0  \n",
      "\n",
      "Data successfully saved to search_console_data_2024-10-11_to_2025-01-09.csv\n"
     ]
    }
   ],
   "source": [
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class SearchConsoleAPI:\n",
    "    def __init__(self):\n",
    "        self.SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']\n",
    "        self.credentials = None\n",
    "        self.service = None\n",
    "        self.ROW_LIMIT = 25000\n",
    "\n",
    "    def authenticate(self, client_secrets_file):\n",
    "        \"\"\"Authenticate using OAuth 2.0\"\"\"\n",
    "        creds = None\n",
    "        token_file = 'token.pickle'\n",
    "        \n",
    "        if os.path.exists(token_file):\n",
    "            with open(token_file, 'rb') as token:\n",
    "                creds = pickle.load(token)\n",
    "\n",
    "        if not creds or not creds.valid:\n",
    "            if creds and creds.expired and creds.refresh_token:\n",
    "                creds.refresh(Request())\n",
    "            else:\n",
    "                flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                    client_secrets_file, self.SCOPES)\n",
    "                creds = flow.run_local_server(port=0)\n",
    "            \n",
    "            with open(token_file, 'wb') as token:\n",
    "                pickle.dump(creds, token)\n",
    "\n",
    "        self.credentials = creds\n",
    "        self.service = build('searchconsole', 'v1', credentials=creds)\n",
    "        return True\n",
    "\n",
    "    def get_site_list(self):\n",
    "        \"\"\"Get list of available sites\"\"\"\n",
    "        try:\n",
    "            sites = self.service.sites().list().execute()\n",
    "            return sites.get('siteEntry', [])\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting sites: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def fetch_data_chunk(self, site_url, start_date, end_date, dimensions, start_row=0):\n",
    "        \"\"\"Fetch a chunk of data\"\"\"\n",
    "        request = {\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'dimensions': dimensions,\n",
    "            'rowLimit': self.ROW_LIMIT,\n",
    "            'startRow': start_row\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = self.service.searchanalytics().query(\n",
    "                siteUrl=site_url,\n",
    "                body=request\n",
    "            ).execute()\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching chunk starting at row {start_row}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def fetch_all_data(self, site_url, start_date, end_date, dimensions, \n",
    "                      max_retries=3, delay_between_chunks=1):\n",
    "        \"\"\"Fetch all data with pagination and retry logic\"\"\"\n",
    "        all_rows = []\n",
    "        start_row = 0\n",
    "        total_rows_fetched = 0\n",
    "        \n",
    "        while True:\n",
    "            retry_count = 0\n",
    "            chunk_data = None\n",
    "            \n",
    "            while retry_count < max_retries and chunk_data is None:\n",
    "                if retry_count > 0:\n",
    "                    print(f\"Retrying chunk (attempt {retry_count + 1}/{max_retries})...\")\n",
    "                    time.sleep(delay_between_chunks * 2)\n",
    "                \n",
    "                chunk_data = self.fetch_data_chunk(\n",
    "                    site_url, start_date, end_date, dimensions, start_row)\n",
    "                retry_count += 1\n",
    "\n",
    "            if chunk_data is None:\n",
    "                print(f\"Failed to fetch chunk after {max_retries} attempts\")\n",
    "                break\n",
    "\n",
    "            rows = chunk_data.get('rows', [])\n",
    "            if not rows:\n",
    "                break\n",
    "\n",
    "            all_rows.extend(rows)\n",
    "            total_rows_fetched += len(rows)\n",
    "            \n",
    "            print(f\"Fetched {len(rows)} rows (Total: {total_rows_fetched})\")\n",
    "            \n",
    "            if len(rows) < self.ROW_LIMIT:\n",
    "                break\n",
    "                \n",
    "            start_row += self.ROW_LIMIT\n",
    "            time.sleep(delay_between_chunks)\n",
    "\n",
    "        return all_rows\n",
    "\n",
    "def process_data(rows, dimensions):\n",
    "    \"\"\"Process the raw data into a DataFrame\"\"\"\n",
    "    processed_rows = []\n",
    "    \n",
    "    for row in rows:\n",
    "        row_data = {}\n",
    "        \n",
    "        # Add dimensions\n",
    "        for i, dimension in enumerate(dimensions):\n",
    "            row_data[dimension] = row['keys'][i]\n",
    "            \n",
    "        # Add metrics\n",
    "        row_data.update({\n",
    "            'clicks': row['clicks'],\n",
    "            'impressions': row['impressions'],\n",
    "            'ctr': row['ctr'],\n",
    "            'position': row['position']\n",
    "        })\n",
    "        \n",
    "        processed_rows.append(row_data)\n",
    "    \n",
    "    return pd.DataFrame(processed_rows)\n",
    "\n",
    "def main():\n",
    "    # Initialize API client\n",
    "    api = SearchConsoleAPI()\n",
    "    \n",
    "    # Your OAuth client secrets file path\n",
    "    client_secrets_file = 'client_secret_81929404806-p1kl4usbb9llq1j4pbvg35a08toljnn3.apps.googleusercontent.com.json'  # Update this path\n",
    "    \n",
    "    # Authenticate\n",
    "    print(\"Authenticating...\")\n",
    "    if not api.authenticate(client_secrets_file):\n",
    "        print(\"Authentication failed\")\n",
    "        return\n",
    "\n",
    "    # Get available sites\n",
    "    sites = api.get_site_list()\n",
    "    if not sites:\n",
    "        print(\"No sites available\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nAvailable sites:\")\n",
    "    for site in sites:\n",
    "        print(f\"- {site['siteUrl']}\")\n",
    "\n",
    "    # Use the correct site URL format\n",
    "    site_url = 'sc-domain:scholistico.com'  # Using the domain property format\n",
    "    \n",
    "    # Set date range (past 90 days)\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    start_date = (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    dimensions = ['date', 'query', 'page', 'device', 'country']\n",
    "\n",
    "    print(f\"\\nFetching all data for {site_url}\")\n",
    "    print(f\"Date range: {start_date} to {end_date}\")\n",
    "    print(f\"Dimensions: {', '.join(dimensions)}\")\n",
    "\n",
    "    # Fetch all data\n",
    "    rows = api.fetch_all_data(\n",
    "        site_url=site_url,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        dimensions=dimensions,\n",
    "        delay_between_chunks=1\n",
    "    )\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No data received\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nTotal rows fetched: {len(rows)}\")\n",
    "\n",
    "    # Process data\n",
    "    print(\"\\nProcessing data...\")\n",
    "    df = process_data(rows, dimensions)\n",
    "\n",
    "    # Save data\n",
    "    output_file = f'search_console_data_{start_date}_to_{end_date}.csv'\n",
    "    print(f\"\\nSaving data to {output_file}...\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(\"\\nData Summary:\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(\"\\nSample of the data:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nData successfully saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df=pd.read_csv('search_console_data_2024-10-11_to_2025-01-09.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/rw/y6kkf2w15czf115nxx_bd3r40000gn/T/ipykernel_35138/2790003395.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  list_of_blogs=df.loc[df.page.str.contains('/\\d+') &  ~(df.page.str.contains('product|blog|groups|course|produkt|cursos|topic|category|shop|page|ref|Corsi|kurser|fr.scholistico|fi.scholistico|dk.scholistico|no.scholistico'))].page.unique()\n"
     ]
    }
   ],
   "source": [
    "list_of_blogs=df.loc[df.page.str.contains('/\\d+') &  ~(df.page.str.contains('product|blog|groups|course|produkt|cursos|topic|category|shop|page|ref|Corsi|kurser|fr.scholistico|fi.scholistico|dk.scholistico|no.scholistico'))].page.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            blog_link    type\n",
      "0   https://scholistico.com/10-best-art-therapy-ac...  manual\n",
      "1   https://scholistico.com/8-best-art-therapy-exe...  manual\n",
      "2   https://it.scholistico.com/10-esercizi-di-arte...  manual\n",
      "3   https://scholistico.com/10-art-therapy-exercis...  manual\n",
      "4   https://it.scholistico.com/7-migliori-esercizi...  manual\n",
      "..                                                ...     ...\n",
      "94  https://es.scholistico.com/7-maneras-de-crear-...      AI\n",
      "95  https://es.scholistico.com/7-ejercicios-de-art...      AI\n",
      "96  https://scholistico.com/8-natural-approaches-t...      AI\n",
      "97  https://de.scholistico.com/7-kunsttherapieuebu...      AI\n",
      "98  https://de.scholistico.com/7-kunsttherapieuebu...      AI\n",
      "\n",
      "[99 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "\n",
    "# Function to check if 'DALL-E prompt:' exists in comments\n",
    "def check_dalle_prompt(url):\n",
    "    try:\n",
    "        # Send a GET request to the blog URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes (4xx or 5xx)\n",
    "        \n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all comments in the HTML\n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "        \n",
    "        # Check if any of the comments contain 'DALL-E prompt:'\n",
    "        for comment in comments:\n",
    "            if 'DALL-E prompt:' in comment:\n",
    "                return \"AI\"\n",
    "        \n",
    "        # If 'DALL-E prompt:' is not found in comments, return \"manual\"\n",
    "        return \"manual\"\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        # If there's an error in fetching the page (e.g., network error, invalid URL)\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "# List of blog URLs to check\n",
    "\n",
    "# Prepare a list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through each blog URL and check for DALL-E prompt in comments\n",
    "for blog in list_of_blogs:\n",
    "    blog_type = check_dalle_prompt(blog)\n",
    "    results.append({\"blog_link\": blog, \"type\": blog_type})\n",
    "\n",
    "# Convert results to a pandas DataFrame\n",
    "df_1 = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_1)\n",
    "\n",
    "# Optionally, save the result to a CSV file\n",
    "df_1.to_csv('blog_check_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(df,df_1,left_on='page',right_on='blog_link',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['blog_link']).to_csv('final_plotly_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'client_email'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use this to get your service account email\u001b[39;00m\n\u001b[1;32m      9\u001b[0m key_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_secret_81929404806-p1kl4usbb9llq1j4pbvg35a08toljnn3.apps.googleusercontent.com.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mprint_service_account_email\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mprint_service_account_email\u001b[0;34m(key_file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(key_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     key_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService Account Email: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mkey_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclient_email\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'client_email'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def print_service_account_email(key_file_path):\n",
    "    with open(key_file_path, 'r') as f:\n",
    "        key_data = json.load(f)\n",
    "        print(f\"Service Account Email: {key_data['client_email']}\")\n",
    "\n",
    "# Use this to get your service account email\n",
    "key_file_path = 'client_secret_81929404806-p1kl4usbb9llq1j4pbvg35a08toljnn3.apps.googleusercontent.com.json'\n",
    "print_service_account_email(key_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
